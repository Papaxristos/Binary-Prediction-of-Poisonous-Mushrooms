import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import matthews_corrcoef
from google.colab import drive
import os
import shutil

# Checks if Google Drive is already connected and disconnects if necessary
if os.path.isdir('/content/drive'):
    print("Unmounting existing /content/drive...")
    drive.flush_and_unmount()
    if os.path.exists('/content/drive'):
        shutil.rmtree('/content/drive')  # Deletes the existing folder

# We connect Google Drive
drive.mount('/content/drive', force_remount=True)

# File paths in Google Drive
file_paths = {
    "train": '/content/drive/MyDrive/Python Projects/Mushrooms/train.csv',
    "test": '/content/drive/MyDrive/Python Projects/Mushrooms/test.csv',
    "submission": '/content/drive/MyDrive/Python Projects/Mushrooms/submission.csv',
    
}
# Checks if Google Drive is already mounted and unmounts if necessary
if os.path.isdir('/content/drive'):
    print("Unmounting the existing /content/drive...")
    drive.flush_and_unmount()
    if os.path.exists('/content/drive'):
        shutil.rmtree('/content/drive')  # Deletes the existing folder

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)

# File paths in Google Drive
file_paths = {
    "train": '/content/drive/MyDrive/Python Projects/Mushrooms/train.csv',
    "test": '/content/drive/MyDrive/Python Projects/Mushrooms/test.csv',
    "submission": '/content/drive/MyDrive/Python Projects/Mushrooms/submission.csv',
}
def check_file_exists(file_path):
    if not os.path.isfile(file_path):
        print(f"Warning: File not found: {file_path}")
    else:
        print(f"File found: {file_path}")

# Check if the files exist
for file_key, file_path in file_paths.items():
    check_file_exists(file_path)

# Optional: If you want to see the contents of the folder
folder_path = '/content/drive/MyDrive/Python Projects/Mushrooms'
print("Folder contents:", os.listdir(folder_path))

# Loading data with detailed error handling
def load_data(file_path, file_name):
    try:
        data = pd.read_csv(file_path)
        print(f"{file_name} loaded successfully with size {data.shape}")
        return data
    except FileNotFoundError:
        raise FileNotFoundError(f"{file_name} not found at path {file_path}")
    except pd.errors.EmptyDataError:
        raise ValueError(f"{file_name} is empty.")
    except pd.errors.ParserError:
        raise ValueError(f"Parsing error in {file_name}.")
    except Exception as e:
        raise RuntimeError(f"An unexpected error occurred while loading {file_name}: {e}")
# Loading data
train_data = load_data(file_paths['train'], "Train Data")
test_data = load_data(file_paths['test'], "Test Data")

# Exploring data
def explore_data(data, data_name, nrows=5):
    print(f"\n{data_name} Head:\n", data.head(nrows))
    print(f"\n{data_name} Tail:\n", data.tail(nrows))
    print(f"\n{data_name} Information:\n")
    data.info()
    print(f"\nMissing data in {data_name}:\n", data.isnull().sum())

# Exploring data
explore_data(train_data, "Train Data")
explore_data(test_data, "Test Data")

# Splitting features and labels
label_column = 'class'  # Naming the 'label' column for flexibility

if label_column not in train_data.columns:
    raise KeyError(f"Training data must contain the column '{label_column}'")

# Splitting features (X) and labels (y)
X = train_data.drop(columns=[label_column])
y = train_data[label_column]
# Check if features and labels are empty
if X.empty or y.empty:
    raise ValueError("Either features or labels are empty. Check your data.")

# Identify categorical and numerical features
categorical_features = X.select_dtypes(include=['object']).columns
numerical_features = X.select_dtypes(include=['number']).columns

# Print information about the feature separation
print(f"Number of categorical features: {len(categorical_features)}")
print(f"Number of numerical features: {len(numerical_features)}")
print(f"Total number of features: {X.shape[1]}")

# Optionally, print the feature names for confirmation
print("\nCategorical Features:", list(categorical_features))
print("Numerical Features:", list(numerical_features))
!pip install xgboost
import pandas as pd
from google.colab import drive
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, matthews_corrcoef
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder
from joblib import dump

# Connecting to Google Drive
drive.mount('/content/drive')

# Defining the correct path for the file
train_data_path = '/content/drive/MyDrive/Python Projects/Mushrooms/train.csv'
train_data = pd.read_csv(train_data_path)
label_column = 'class'  # Label column

# Splitting features (X) and label (y)
X = train_data.drop(columns=[label_column])
y = train_data[label_column]

# Converting categorical labels to numerical
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Defining features
categorical_features = X.select_dtypes(include=['object']).columns  # Finding categorical features
numerical_features = X.select_dtypes(include=['number']).columns  # Finding numerical features

# Creating transformers
preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler())
        ]), numerical_features),
        ('cat', Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(handle_unknown='ignore'))
        ]), categorical_features)
    ])

# Defining the initial model with XGBoost
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42))
])

# Training the model with the data
model.fit(X, y_encoded)

# Evaluating the model
y_pred_encoded = model.predict(X)
y_pred = label_encoder.inverse_transform(y_pred_encoded)  # Reversing the encoding
y_prob = model.predict_proba(X)[:, 1]  # Probabilities for AUC calculation

# Calculating evaluation metrics
accuracy = accuracy_score(y, y_pred)
print(f"Accuracy: {accuracy:.4f}")

print("\nClassification Report:\n", classification_report(y, y_pred))

roc_auc = roc_auc_score(y_encoded, y_prob)
print(f"ROC AUC: {roc_auc:.4f}")

mcc = matthews_corrcoef(y_encoded, y_pred_encoded)
print(f"Matthews Corrcoef: {mcc:.4f}")

# Defining the path to save the model
model_path = '/content/drive/MyDrive/Python Projects/Mushrooms/trained_model.joblib'

# Saving the trained model
dump(model, model_path)
print(f"Model saved to: {model_path}")
from joblib import dump
import xgboost as xgb
import pandas as pd
from google.colab import drive
from sklearn.preprocessing import LabelEncoder

# Connecting to Google Drive
drive.mount('/content/drive')

# Loading the training data
train_data = pd.read_csv('/content/drive/MyDrive/Python Projects/Mushrooms/train.csv')

# Checking the columns of the dataframe
print("Columns in the CSV file:", train_data.columns)

# Converting categorical columns to numerical values
label_encoders = {}
for column in train_data.columns:
    if train_data[column].dtype == 'object':
        le = LabelEncoder()
        train_data[column] = le.fit_transform(train_data[column])
        label_encoders
from joblib import load
import pandas as pd
import os
from google.colab import drive
from sklearn.exceptions import NotFittedError
from xgboost import XGBClassifier

# Connecting to Google Drive
drive.mount('/content/drive', force_remount=True)

# Defining paths
model_path = '/content/drive/MyDrive/Python Projects/Mushrooms/trained_model.joblib'
test_data_path = '/content/drive/MyDrive/Python Projects/Mushrooms/test.csv'
submission_dir = '/content/drive/MyDrive/Python Projects/Mushrooms/'

# Checking if the model file exists in the directory
if not os.path.exists(model_path):
    print("Files in the directory are:", os.listdir(os.path.dirname(model_path)))
    raise FileNotFoundError(f"The file {model_path} was not found.")

# Loading the trained model
try:
    model = load(model_path)
    # Verifying if the model is a Pipeline
    if hasattr(model, 'named_steps'):
        classifier = model.named_steps.get('classifier')
        if isinstance(classifier, XGBClassifier):
            print("The model includes XGBClassifier.")
        else:
            raise RuntimeError("The model does not contain XGBClassifier.")
    else:
        raise RuntimeError("The model is not a Pipeline or does not contain named_steps.")
    print("The model was loaded successfully.")
except Exception as e:
    raise RuntimeError(f"Error loading the model: {e}")

# Verifying if the model is trained
try:
    # Use columns from training data to create dummy input
    test_data = pd.read_csv(test_data_path)
    dummy_input = test_data.head(1)  # Use the first few rows of data as dummy input

    # Generate a prediction to confirm the model
    model.predict(dummy_input)
    print("The model is trained and ready for predictions.")
except NotFittedError as e:
    raise RuntimeError("The model does not appear to be properly trained. Please use `fit` or `load_model` to train it first.")
except Exception as e:
    raise RuntimeError(f"Error confirming the model: {e}")
# Loading the test data
test_data = pd.read_csv(test_data_path)

# Checking if the 'id' column exists
if 'id' not in test_data.columns:
    raise KeyError("Test data must contain 'id' column")

# Removing the 'id' column from the data
X_test = test_data.drop(columns=['id'], errors='ignore')

# Creating a list of columns that were used when training the model
expected_columns = [
    'cap-diameter', 'cap-shape', 'cap-surface', 'cap-color',
    'does-bruise-or-bleed', 'gill-attachment', 'gill-spacing', 'gill-color',
    'stem-height', 'stem-width', 'stem-root', 'stem-surface', 'stem-color',
    'veil-type', 'veil-color', 'has-ring', 'ring-type', 'spore-print-color',
    'habitat', 'season'
]

# Add missing columns with zeros (or other default values)
import pandas as pd
from sklearn.pipeline import Pipeline

# Loading the test data
test_data = pd.read_csv(test_data_path)

# Checking if the 'id' column exists
if 'id' not in test_data.columns:
    raise KeyError("Test data must contain 'id' column")

# Removing the 'id' column from the data
X_test = test_data.drop(columns=['id'])

# Displaying the first rows of X_test for debugging
print("First rows of X_test:")
print(X_test.head())

try:
    # Applying the pipeline to X_test
    preprocessor = model.named_steps['preprocessor']
    
    # Displaying the columns recognized by the preprocessor
    print("Columns recognized by the preprocessor:")
    print(preprocessor.transformers_[0][1].get_feature_names_out())  # Displaying feature names

    # Applying preprocessing to X_test
    X_test_transformed = preprocessor.transform(X_test)

    # Making predictions with the trained model
    classifier = model.named_steps['classifier']
    predictions = classifier.predict(X_test_transformed)

    # Creating the DataFrame for submission
    submission = pd.DataFrame({
        'id': test_data['id'],  # Restoring the 'id' column in the submission
        'class': predictions    # Creating the 'class' column with predictions
    })

    # Defining the path to save the submission
    submission_path = '/content/drive/MyDrive/Python Projects/Mushrooms/submission.csv'
    submission.to_csv(submission_path, index=False)
    print(f"Submission saved to: {submission_path}")

except KeyError as ke:
    raise RuntimeError(f"Error: {ke}")
except Exception as e:
    raise RuntimeError(f"Error processing test data: {e}")
import os
import pandas as pd

# Path settings
submission_dir = '/content/drive/MyDrive/Python Projects/Mushrooms'
submission_path = os.path.join(submission_dir, 'submission.csv')
score_file_path = os.path.join(submission_dir, 'mcc_score.csv')

# Create the directory if it does not exist
os.makedirs(submission_dir, exist_ok=True)

# Test data processing
def preprocess_test_data(test_data, model):
    # Check if the 'id' column exists
    if 'id' not in test_data.columns:
        raise KeyError("Test data must contain 'id' column")

    # Remove the 'id' column from the data
    X_test = test_data.drop(columns=['id'], errors='ignore')  # Using errors='ignore' to avoid errors if it does not exist

    # Ensure that the columns of X_test include the expected columns
    try:
        expected_columns = model.feature_names_in_
    except AttributeError:
        raise RuntimeError("The model does not provide the feature_names_in_ attribute. Ensure that you have the correct columns.")

    print("Expected model columns:", expected_columns)

    # Ensure that the columns of X_test include the columns expected by the model
    missing_columns = [col for col in expected_columns if col not in X_test.columns]

    # Add the missing columns with zeros (or other default values)
    for column in missing_columns:
        X_test[column] = 0  # You can use another default value here if needed

    # Reorder columns to match the model's expected columns
    X_test = X_test[expected_columns]

    return X_test

# Example usage of the function
X_test = preprocess_test_data(test_data, model)

# Assume predictions are made and the submission DataFrame is created
# For demonstration purposes, we assume `predictions` is a list or array of predicted values
predictions = model.predict(X_test)  # Replace with actual prediction code

# Create the submission DataFrame
submission = pd.DataFrame({
    'id': test_data['id'],  # Assuming 'id' is still available in `test_data`
    'class': predictions    # Predictions made by the model
})

# Save the submission file
submission.to_csv(submission_path, index=False)
print("The submission file has been successfully created.")
